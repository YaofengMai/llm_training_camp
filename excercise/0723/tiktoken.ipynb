{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n"
      ],
      "metadata": {
        "id": "FfJMWi2JGGR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "91s31c39GOR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.encode(\"tiktoken is great!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5Nr4thnGSAK",
        "outputId": "3cfddea2-047e-4be4-fb6c-6682ed7c2216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[83, 1609, 5963, 374, 2294, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"返回文本字符串中的Token数量\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens"
      ],
      "metadata": {
        "id": "WhXZhHJkGYUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1TnQC-QGcTa",
        "outputId": "589532a2-051b-4f95-b5dc-0f95ded3beb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens_from_string(\"tiktoken is great!\", \"p50k_base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGxm005tGiyx",
        "outputId": "f2b259b8-a40e-4a0c-df0b-0a931f91a713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.decode([83, 1609, 5963, 374, 2294, 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PG4o1ZzwGx9a",
        "outputId": "e5b74a87-666a-441b-f471-cd018a904a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tiktoken is great!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.encode(\"今天天气真好\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Qny9WEDG0WJ",
        "outputId": "7758e126-fb1e-4005-cac0-f0ebb9cd730b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[37271, 36827, 36827, 30320, 242, 89151, 53901]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.decode([37271, 36827, 36827, 30320, 242, 89151, 53901])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CcP42mHRG4SS",
        "outputId": "f6da3ef3-4eb3-42cd-aebb-dc556264653d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'今天天气真好'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[encoding.decode_single_token_bytes(token) for token in [83, 1609, 5963, 374, 2294, 0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZA2xfu5HC7R",
        "outputId": "e79d7229-4f02-4c3b-ac7c-7b461d065a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b't', b'ik', b'token', b' is', b' great', b'!']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[encoding.decode_single_token_bytes(token) for token in [37271, 36827, 36827, 30320, 242, 89151, 53901]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWyTRcRwHExh",
        "outputId": "58774cc8-08e5-4f5f-8767-879b83e19f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b'\\xe4\\xbb\\x8a',\n",
              " b'\\xe5\\xa4\\xa9',\n",
              " b'\\xe5\\xa4\\xa9',\n",
              " b'\\xe6\\xb0',\n",
              " b'\\x94',\n",
              " b'\\xe7\\x9c\\x9f',\n",
              " b'\\xe5\\xa5\\xbd']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [encoding.decode_single_token_bytes(token) for token in [37271, 36827, 36827, 30320, 242, 89151, 53901]]:\n",
        "  try:\n",
        "    print(i.decode('utf-8'))\n",
        "  except:\n",
        "    print(i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJh1XlheHUBy",
        "outputId": "7c09e83d-87fd-4d0f-e71e-8d8fefe8f8f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "今\n",
            "天\n",
            "天\n",
            "b'\\xe6\\xb0'\n",
            "b'\\x94'\n",
            "真\n",
            "好\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = b'\\xe6\\xb0\\x94'\n",
        "decoded_str = data.decode('utf-8')\n",
        "print(decoded_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l8Q1gFwH0kp",
        "outputId": "e29655ed-ceca-47ca-9be0-5ad2362b778f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "气\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "比较不同的编码器"
      ],
      "metadata": {
        "id": "wyMyvQI5Ik9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_encodings(example_string: str) -> None:\n",
        "    \"\"\"Prints a comparison of three string encodings.\"\"\"\n",
        "    # print the example string\n",
        "    print(f'\\nExample string: \"{example_string}\"')\n",
        "    # for each encoding, print the # of tokens, the token integers, and the token bytes\n",
        "    for encoding_name in [\"gpt2\", \"p50k_base\", \"cl100k_base\"]:\n",
        "        encoding = tiktoken.get_encoding(encoding_name)\n",
        "        token_integers = encoding.encode(example_string)\n",
        "        num_tokens = len(token_integers)\n",
        "        token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\n",
        "        print()\n",
        "        print(f\"{encoding_name}: {num_tokens} tokens\")\n",
        "        print(f\"token integers: {token_integers}\")\n",
        "        print(f\"token bytes: {token_bytes}\")"
      ],
      "metadata": {
        "id": "9_8fTCmeImxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_encodings(\"antidisestablishmentarianism\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykh6FaRXIqF9",
        "outputId": "7837c3d3-34c8-4e38-a1d6-749984b92b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example string: \"antidisestablishmentarianism\"\n",
            "\n",
            "gpt2: 5 tokens\n",
            "token integers: [415, 29207, 44390, 3699, 1042]\n",
            "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n",
            "\n",
            "p50k_base: 5 tokens\n",
            "token integers: [415, 29207, 44390, 3699, 1042]\n",
            "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n",
            "\n",
            "cl100k_base: 6 tokens\n",
            "token integers: [519, 85342, 34500, 479, 8997, 2191]\n",
            "token bytes: [b'ant', b'idis', b'establish', b'ment', b'arian', b'ism']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compare_encodings(\"6+20=26\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dSTXwhhItaB",
        "outputId": "1b1e278d-4098-4eb8-a0cf-e7998c23c1b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example string: \"6+20=26\"\n",
            "\n",
            "gpt2: 5 tokens\n",
            "token integers: [21, 10, 1238, 28, 2075]\n",
            "token bytes: [b'6', b'+', b'20', b'=', b'26']\n",
            "\n",
            "p50k_base: 5 tokens\n",
            "token integers: [21, 10, 1238, 28, 2075]\n",
            "token bytes: [b'6', b'+', b'20', b'=', b'26']\n",
            "\n",
            "cl100k_base: 5 tokens\n",
            "token integers: [21, 10, 508, 28, 1627]\n",
            "token bytes: [b'6', b'+', b'20', b'=', b'26']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compare_encodings(\"今天天气真好\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE96klcyI0Ir",
        "outputId": "f17542a8-e1fe-453c-c796-af83e4ee1a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example string: \"今天天气真好\"\n",
            "\n",
            "gpt2: 10 tokens\n",
            "token integers: [20015, 232, 25465, 25465, 36365, 242, 40367, 253, 25001, 121]\n",
            "token bytes: [b'\\xe4\\xbb', b'\\x8a', b'\\xe5\\xa4\\xa9', b'\\xe5\\xa4\\xa9', b'\\xe6\\xb0', b'\\x94', b'\\xe7\\x9c', b'\\x9f', b'\\xe5\\xa5', b'\\xbd']\n",
            "\n",
            "p50k_base: 10 tokens\n",
            "token integers: [20015, 232, 25465, 25465, 36365, 242, 40367, 253, 25001, 121]\n",
            "token bytes: [b'\\xe4\\xbb', b'\\x8a', b'\\xe5\\xa4\\xa9', b'\\xe5\\xa4\\xa9', b'\\xe6\\xb0', b'\\x94', b'\\xe7\\x9c', b'\\x9f', b'\\xe5\\xa5', b'\\xbd']\n",
            "\n",
            "cl100k_base: 7 tokens\n",
            "token integers: [37271, 36827, 36827, 30320, 242, 89151, 53901]\n",
            "token bytes: [b'\\xe4\\xbb\\x8a', b'\\xe5\\xa4\\xa9', b'\\xe5\\xa4\\xa9', b'\\xe6\\xb0', b'\\x94', b'\\xe7\\x9c\\x9f', b'\\xe5\\xa5\\xbd']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义函数 num_tokens_from_messages，该函数返回由一组消息所使用的token数。\n",
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
        "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
        "    # 尝试获取模型的编码\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        # 如果模型没有找到，使用 cl100k_base 编码并给出警告\n",
        "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    # 针对不同的模型设置token数量\n",
        "    if model in {\n",
        "        \"gpt-3.5-turbo-0613\",\n",
        "        \"gpt-3.5-turbo-16k-0613\",\n",
        "        }:\n",
        "        tokens_per_message = 3\n",
        "        tokens_per_name = 1\n",
        "    elif model == \"gpt-3.5-turbo-0301\":\n",
        "        tokens_per_message = 4  # 每条消息遵循 {role/name}\\n{content}\\n 格式\n",
        "        tokens_per_name = -1  # 如果有名字，角色会被省略\n",
        "    elif \"gpt-3.5-turbo\" in model:\n",
        "        # 对于 gpt-3.5-turbo 模型可能会有更新，此处返回假设为 gpt-3.5-turbo-0613 的token数量，并给出警告\n",
        "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
        "    else:\n",
        "        # 对于没有实现的模型，抛出未实现错误\n",
        "        raise NotImplementedError(\n",
        "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
        "        )\n",
        "    num_tokens = 0\n",
        "    # 计算每条消息的token数\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3  # 每条回复都以助手为首\n",
        "    return num_tokens"
      ],
      "metadata": {
        "id": "JLy2hmOGJmn6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 让我们验证上面的函数是否与OpenAI API的响应匹配\n",
        "\n",
        "import openai\n",
        "\n",
        "example_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_user\",\n",
        "        \"content\": \"New synergies will help drive top-line growth.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_assistant\",\n",
        "        \"content\": \"Things working well together will increase revenue.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_user\",\n",
        "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_assistant\",\n",
        "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
        "    },\n",
        "]\n",
        "\n",
        "for model in [\n",
        "    \"gpt-3.5-turbo-0613\",\n",
        "    \"gpt-3.5-turbo\",\n",
        "    \"gpt-3.5-turbo-0301\",\n",
        "    ]:\n",
        "    print(model)\n",
        "    # example token count from the function defined above\n",
        "    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n",
        "    # example token count from the OpenAI API\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=example_messages,\n",
        "        temperature=0,\n",
        "        max_tokens=1,  # we're only counting input tokens here, so let's not waste tokens on the output\n",
        "    )\n",
        "    print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens counted by the OpenAI API.')\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNC3uCV8J78i",
        "outputId": "115c2da8-ce4e-4eac-9d2e-dedeb58db4e9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-3.5-turbo-0613\n",
            "129 prompt tokens counted by num_tokens_from_messages().\n",
            "129 prompt tokens counted by the OpenAI API.\n",
            "\n",
            "gpt-3.5-turbo\n",
            "Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\n",
            "129 prompt tokens counted by num_tokens_from_messages().\n",
            "129 prompt tokens counted by the OpenAI API.\n",
            "\n",
            "gpt-3.5-turbo-0301\n",
            "127 prompt tokens counted by num_tokens_from_messages().\n",
            "127 prompt tokens counted by the OpenAI API.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}